{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README:\n",
    "This is the python parameterization pipeline that we used to analyze the behavior of our two recommendation methods. The beginning of each cell has a comment at the top that explains the purpose of the cell. However, a brief synopsis is provided here as well.\n",
    "The only supporting code this pynotebook uses are listed in the \"import\" cell (directly below this cell)\n",
    "\n",
    "1: Translate the MATLAB functions into python\n",
    "\n",
    "2: Define two python Classes in terms of the translated functions, so that the two methods can be tested and compared using scikit-learn's GridSearchCV function\n",
    "\n",
    "3: Load the four datasets that were pre-processed in MATLAB\n",
    "\n",
    "4: Run GridSearchCV on the two different methods with each of the four datasets, varying the rank value for the NNMF method, and the fraction of variance explained (FoV) for the PCA by SVD method.\n",
    "\n",
    "5: Report the resulting RMSE values for each parameter in both methods, four all four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "import scipy.io\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the python implementations of the MATLAB functions we used for our recommendation system.\n",
    "# As far as we have tested, they are the same as the MATLAB functions, though with different syntax\n",
    "# for obvious reasons.\n",
    "# Because these functions are just coppied over from MATLAB, they are left uncommented\n",
    "\n",
    "def svd_deconstruction(X, defaultVals, colMeans, rowMeans):        \n",
    "    if (colMeans == 0 and rowMeans == 0):\n",
    "        X[X==0] = defaultVals\n",
    "        \n",
    "    elif (colMeans == 1 and rowMeans == 0):\n",
    "       for k in range(len(defaultVals)):\n",
    "           logicalIndex = X[:,k] == 0\n",
    "           X[logicalIndex] = defaultVals[k]\n",
    "       \n",
    "    elif (colMeans == 0 and rowMeans == 1):\n",
    "        for k in range(len(defaultVals)):\n",
    "           logicalIndex = X[k,:] == 0\n",
    "           X[logicalIndex] = defaultVals[k]\n",
    "           \n",
    "    userCount = X.shape[1]\n",
    "    meanVec = X.mean()\n",
    "    xTilde = X - meanVec\n",
    "    xTildeScaled = (1/np.sqrt(userCount))*xTilde\n",
    "    W, S, V = np.linalg.svd(xTildeScaled, full_matrices=False)\n",
    "    pcMatrix = np.matmul(np.transpose(W), xTilde)\n",
    "    lambdas = np.multiply(S, S)\n",
    "    sortedLambdaVec = np.diag(lambdas)\n",
    "    return pcMatrix, W, sortedLambdaVec\n",
    "\n",
    "\n",
    "def KPrincipled(eigenvalueVector, principledThreshold):\n",
    "    fractionOfVariance = np.zeros(len(eigenvalueVector))\n",
    "    total = np.sum(eigenvalueVector)\n",
    "    cummulative = np.cumsum(eigenvalueVector)\n",
    "    fractionOfVariance = np.divide(cummulative, total)\n",
    "    \n",
    "    idx = 0\n",
    "    while fractionOfVariance[idx] < principledThreshold:\n",
    "        idx += 1\n",
    "    \n",
    "    return idx\n",
    "\n",
    "\n",
    "def lowRankMatrixApproximator(muX, W, pcMatrix, k):\n",
    "    predictedListensMatrix = np.transpose(muX + np.matmul(W[:,:k], pcMatrix[:k,:]))\n",
    "    predictedListensMatrix[predictedListensMatrix < 0] = 0\n",
    "    \n",
    "    return predictedListensMatrix\n",
    "\n",
    "\n",
    "def recommend(X, threshold = 0.9, defaultVals=0, colMeans=0, rowMeans=0):\n",
    "    pcMatrix, W, sortedLambdaVec = svd_deconstruction(X, defaultVals, colMeans, rowMeans)\n",
    "    principledK = KPrincipled(sortedLambdaVec, threshold)\n",
    "    predictedListensMatrix = lowRankMatrixApproximator(X.mean(), W, pcMatrix, principledK)\n",
    "    return predictedListensMatrix\n",
    "\n",
    "\n",
    "def listen2Rating(X):\n",
    "    newX = np.zeros(X.shape)\n",
    "    for k in range(X.shape[1]):\n",
    "        bins = np.histogram(X[:,k], bins=5)\n",
    "        newX[:,k] = bins\n",
    "    \n",
    "    return newX\n",
    "\n",
    "\n",
    "def normData(X):\n",
    "    u = X\n",
    "    u[u == 0] = np.nan\n",
    "    newX = np.zeros(X.shape)\n",
    "    meanX = X.mean(axis=0)\n",
    "    stdX = np.std(X, axis=0)\n",
    "    \n",
    "    for k in range(X.shape[1]):\n",
    "        currentMean = meanX[k]\n",
    "        currentStd = stdX[k]\n",
    "        threshold = 3*currentStd + currentMean\n",
    "        for ii in range(X.shape[0]):\n",
    "            if u[ii,k] > threshold:\n",
    "                newX[ii,k] = currentMean\n",
    "            else:\n",
    "                newX[ii,k] = u[ii,k]\n",
    "                \n",
    "    return (newX - np.min(newX))/(np.max(newX) - np.min(newX))\n",
    "\n",
    "\n",
    "def reconstructionAndError(mat, randomRemovals=4, rank=9, iterations=10):\n",
    "    u = mat\n",
    "    length = mat.shape[0]\n",
    "    width = mat.shape[1]\n",
    "\n",
    "    t = u\n",
    "    t[np.isnan(t)] = 0\n",
    "    \n",
    "    estimator = NMF(n_components=rank, solver='mu')\n",
    "    w = estimator.fit_transform(t)\n",
    "    h = estimator.components_\n",
    "    predicted = np.dot(w,h)\n",
    "    \n",
    "    removalVec = np.zeros((randomRemovals,2))\n",
    "    t2 = t\n",
    "    avgRMSE = np.zeros((length,iterations))\n",
    "    w2 = estimator.fit_transform(t2)\n",
    "    h2 = estimator.components_\n",
    "    predicted2 = np.dot(w2,h2)\n",
    "    \n",
    "    for k in range(iterations):\n",
    "        count = 1\n",
    "        skip = False\n",
    "        \n",
    "        while count < (randomRemovals):\n",
    "            randX = rand.randint(0, length-1)\n",
    "            randY = rand.randint(0, width-1)\n",
    "            if not(np.isnan(t2[randX,randY])):\n",
    "                removalVec[count,:] = [randX, randY]\n",
    "                t2[randX, randY] = 0\n",
    "                skip = True\n",
    "            if skip:\n",
    "                count = count+1\n",
    "                skip = False\n",
    "        \n",
    "        close = 0\n",
    "        for ii in range(randomRemovals):\n",
    "            close = close + np.linalg.norm(\n",
    "                t[int(removalVec[ii,0]),int(removalVec[ii,1])] - predicted2[int(removalVec[ii,0]),int(removalVec[ii,1])])\n",
    "        avgRMSE[k] = np.sqrt(close/randomRemovals)\n",
    "    \n",
    "    finalRMSE = avgRMSE.mean()\n",
    "    return w, h, predicted, finalRMSE\n",
    "\n",
    "\n",
    "def reconstructionAndErrorSVD(mat, randomRemovals=4, threshold=0.9, iterations=10):\n",
    "    u = mat\n",
    "    length = mat.shape[0]\n",
    "    width = mat.shape[1]\n",
    "\n",
    "    t = u\n",
    "    t[np.isnan(t)] = 0\n",
    "    \n",
    "    # estimator = NMF(n_components=rank, solver='mu')\n",
    "    # w = estimator.fit_transform(t)\n",
    "    # h = estimator.components_\n",
    "    # predicted = np.dot(w,h)\n",
    "    predicted = np.transpose(recommend(t,threshold=threshold))\n",
    "    predicted2 = predicted.copy()\n",
    "    \n",
    "    removalVec = np.zeros((randomRemovals,2))\n",
    "    t2 = t\n",
    "    avgRMSE = np.zeros((length,iterations))\n",
    "    # w2 = estimator.fit_transform(t2)\n",
    "    # h2 = estimator.components_\n",
    "    # predicted2 = np.dot(w2,h2)\n",
    "    #predicted2 = recommend(t2,threshold=threshold)\n",
    "    \n",
    "    for k in range(iterations):\n",
    "        count = 0\n",
    "        skip = False\n",
    "        \n",
    "        while count < (randomRemovals):\n",
    "            randX = rand.randint(0, length-1)\n",
    "            randY = rand.randint(0, width-1)\n",
    "            if not(np.isnan(t2[randX,randY])):\n",
    "                removalVec[count,:] = [randX, randY]\n",
    "                t2[randX, randY] = 0\n",
    "                skip = True\n",
    "            if skip:\n",
    "                count = count+1\n",
    "                skip = False\n",
    "        \n",
    "        close = 0\n",
    "        for ii in range(randomRemovals):\n",
    "            a = t[int(removalVec[ii,0]),int(removalVec[ii,1])]\n",
    "            b = predicted2[int(removalVec[ii,0]),int(removalVec[ii,1])]\n",
    "            close = close + np.linalg.norm(a - b)\n",
    "        avgRMSE[k] = np.sqrt(close/randomRemovals)\n",
    "    \n",
    "    finalRMSE = avgRMSE.mean()\n",
    "    return predicted, finalRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains two bare-bones classes that implement the two versions of our recommendation\n",
    "# system (the NNMF version and the PCA by SVD version)\n",
    "# The classes have all of the functions needed to test our implementations using GridSearchCV.\n",
    "# An important note: GridSearchCV assumes that the highest score is the best. We're using the \n",
    "# RMSE between the original data and the reconstructed data, which means our \"best\" parameters\n",
    "# will be the lowest. For this reason, the RMSE values are negated before they're returned.\n",
    "# (the score() function returns -self.finalRMSE, not +self.finalRMSE)\n",
    "\n",
    "class Recommend:\n",
    "    def __init__(self, n_components=10, randomRemovals=4, iterations=10):\n",
    "        self.rank = n_components\n",
    "        self.randomRemovals = randomRemovals\n",
    "        self.iterations = iterations\n",
    "        return\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        w, h, predicted, finalRMSE = reconstructionAndError(X, randomRemovals=self.randomRemovals,iterations=self.iterations )\n",
    "        self.finalRMSE = finalRMSE\n",
    "        self.W = w\n",
    "        self.h = h\n",
    "        self.predicted = predicted\n",
    "        return\n",
    "    \n",
    "    def transform(self,X):\n",
    "        w, h, predicted, finalRMSE = reconstructionAndError(X, randomRemovals=self.randomRemovals,iterations=self.iterations )\n",
    "        self.finalRMSE = finalRMSE\n",
    "        self.W = w\n",
    "        self.h = h\n",
    "        self.predicted = predicted\n",
    "        return\n",
    "    \n",
    "    def set_params(self,n_components=10, randomRemovals=4, iterations=10):\n",
    "        self.rank = n_components\n",
    "        self.randomRemovals = randomRemovals\n",
    "        self.iterations = iterations\n",
    "        # keys = params.keys()\n",
    "        # if 'rank' in keys:\n",
    "        #     Recommend.rank = params['rank']\n",
    "        # if 'n_components' in keys:\n",
    "        #     Recommend.rank = params['n_components']\n",
    "        # if 'randomRemovals' in keys:\n",
    "        #     Recommend.randomRemovals = params['randomRemovals']\n",
    "        # if 'iterations' in keys:\n",
    "        #     Recommend.iterations = params['iterations']\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        return -self.finalRMSE\n",
    "\n",
    "class RecommendSVD:\n",
    "    def __init__(self, n_components=10, randomRemovals=4, iterations=10, threshold=0.9):\n",
    "        self.rank = n_components\n",
    "        self.randomRemovals = randomRemovals\n",
    "        self.iterations = iterations\n",
    "        self.threshold = threshold\n",
    "        return\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        predicted, finalRMSE = reconstructionAndErrorSVD(\n",
    "            X, randomRemovals=self.randomRemovals,iterations=self.iterations, threshold=self.threshold )\n",
    "        self.finalRMSE = finalRMSE\n",
    "        self.predicted = predicted\n",
    "        return\n",
    "    \n",
    "    def transform(self,X):\n",
    "        predicted, finalRMSE = reconstructionAndErrorSVD(\n",
    "            X, randomRemovals=self.randomRemovals,iterations=self.iterations, threshold=self.threshold )\n",
    "        self.finalRMSE = finalRMSE\n",
    "        self.predicted = predicted\n",
    "        return\n",
    "    \n",
    "    def set_params(self,n_components=10, randomRemovals=4, iterations=10, threshold=0.9):\n",
    "        self.rank = n_components\n",
    "        self.randomRemovals = randomRemovals\n",
    "        self.iterations = iterations\n",
    "        self.threshold=threshold\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        return -self.finalRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the matlab-processed data, then split the data into train and test sets\n",
    "dataSets = {}\n",
    "trainTestSets = {}\n",
    "for k in [5, 10, 15, 20]:\n",
    "    # Construct file name (without extension)\n",
    "    currentFile = 'UserArtists_' + str(k)\n",
    "    \n",
    "    # Load and store the data (there is only one variable in the file, ua_adj_transpose)\n",
    "    currentSet = scipy.io.loadmat(currentFile+'.mat')['ua_adj_transpose']\n",
    "    dataSets[currentFile] = currentSet\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    xtrain, xtest = train_test_split(currentSet, test_size=0.2)\n",
    "    currentSplit = {'train': xtrain, 'test': xtest}\n",
    "    \n",
    "    # trainTestSets is a dictionary of dictionaries. Each dictionary it holds corresponds to\n",
    "    # a train-test split of one of our four test datasets.\n",
    "    trainTestSets[currentFile] = currentSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1346)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what dataSets holds (a 100xN array).\n",
    "# The train/test splits are the same (a 100x0.8N matrix for training, 100x0.2N for testing)\n",
    "dataSets['UserArtists_5'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the parameterization goes. To test each method, just comment/uncomment the associated\n",
    "# block. (This is being submitted with RecommendSVD() commented out, and Recommend() active)\n",
    "\n",
    "# Pipelines are mutable (or at least functionally mutable when used with GridsearchCV), but they \n",
    "# need to be initialized to be used in a parameter grid search.\n",
    "# The initialization is a set of ('name', Operation()) pairs. The 'name's are preserved \n",
    "# throughout the grid search, but the Operation()s are swapped out in an exaustive combination.\n",
    "# Our pipeline only reduces the data (either by NNMF or PCA by SVD), so we include a field \n",
    "# called 'reducer'\n",
    "\n",
    "pipeline = Pipeline((\n",
    "    ('reducer', PCA()),\n",
    "))\n",
    "\n",
    "N_FEATURES_OPTIONS = [20, 40, 60, 80, 100]\n",
    "THRESHOLDS = [0.1, 0.4, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "param_grid = [\n",
    "    # {\n",
    "    #     'reducer': [RecommendSVD()],\n",
    "    #     'reducer__threshold': THRESHOLDS,\n",
    "    # },\n",
    "    {\n",
    "        'reducer': [Recommend()],\n",
    "        'reducer__n_components': N_FEATURES_OPTIONS,\n",
    "    },\n",
    "]\n",
    "grid = GridSearchCV(pipeline, param_grid, n_jobs=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cv results for each dataset\n",
    "\n",
    "# Holds all of the results and information for each dataset (can be slightly difficult to navigate)\n",
    "gridResults = {}\n",
    "\n",
    "# Holds the best parameter combinations for each dataset\n",
    "# (the parameters for which results are reported)\n",
    "bestCombinations = {}\n",
    "\n",
    "# Holds the training cross-validation scores using the best parameters\n",
    "# (conducted separately from the cvs in gridResults)\n",
    "trainScores = {}\n",
    "\n",
    "# Holds the testing cross-validation scores using the best parameters\n",
    "testScores = {}\n",
    "for key in trainTestSets.keys():\n",
    "    xtrain = trainTestSets[key]['train']\n",
    "    xtest = trainTestSets[key]['test']\n",
    "    \n",
    "    # This is where the exhaustive search is conducted. After .fit() returns, the \n",
    "    # results are in cv_results_, which holds 5-fold cv-scores for each combination\n",
    "    # of parameters. The parameters that lead to the lowest RMSE are considdered\n",
    "    # the \"best\" parameters, however, just because we have a low RMSE does not mean\n",
    "    # our machine makes good music recommendations (that would require user testing)\n",
    "    grid.fit(xtrain)\n",
    "    gridResults[key] = pd.DataFrame(grid.cv_results_)\n",
    "    bestCombinations[key] = grid.best_estimator_\n",
    "    trainScores[key] = cross_val_score(grid.best_estimator_, xtrain, cv=5)\n",
    "    testScores[key] = cross_val_score(grid.best_estimator_, xtest, cv=5)\n",
    "    \n",
    "testScores = pd.DataFrame(testScores)\n",
    "trainScores = pd.DataFrame(trainScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserArtists_5\n",
      "Best rank: 0.4\n",
      "Test data RMSE\n",
      "0   -16.889572\n",
      "1    -4.285606\n",
      "2    -9.440983\n",
      "3   -11.177004\n",
      "4    -9.745557\n",
      "Name: UserArtists_5, dtype: float64\n",
      "Mean RMSE: -10.307744530776986\n",
      "Std RMSE: 4.035084744190852\n",
      "\n",
      "UserArtists_10\n",
      "Best rank: 0.99\n",
      "Test data RMSE\n",
      "0    -6.693916\n",
      "1    -4.307130\n",
      "2    -4.359845\n",
      "3    -2.999081\n",
      "4   -11.764841\n",
      "Name: UserArtists_10, dtype: float64\n",
      "Mean RMSE: -6.02496244340764\n",
      "Std RMSE: 3.1071925970644347\n",
      "\n",
      "UserArtists_15\n",
      "Best rank: 0.95\n",
      "Test data RMSE\n",
      "0   -4.663890\n",
      "1   -2.196229\n",
      "2   -2.916414\n",
      "3   -2.242560\n",
      "4   -8.150470\n",
      "Name: UserArtists_15, dtype: float64\n",
      "Mean RMSE: -4.03391260066157\n",
      "Std RMSE: 2.244003038903447\n",
      "\n",
      "UserArtists_20\n",
      "Best rank: 0.95\n",
      "Test data RMSE\n",
      "0    -6.910756\n",
      "1    -4.740739\n",
      "2    -7.372109\n",
      "3   -10.940431\n",
      "4    -7.821062\n",
      "Name: UserArtists_20, dtype: float64\n",
      "Mean RMSE: -7.5570192606693185\n",
      "Std RMSE: 1.9950176313280674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display relevant CV scores (values are negative for GridSearchCV, interpret them as positive)\n",
    "for key in testScores.keys():\n",
    "    print(key)\n",
    "    #print(\"Best FoV Threshold: \" + str(bestCombinations[key]['reducer'].threshold))\n",
    "    print(\"Best rank: \" + str(bestCombinations[key]['reducer'].rank))\n",
    "    print(\"Test data RMSE\")\n",
    "    print(testScores[key])\n",
    "    print(\"Mean RMSE: \" + str(testScores[key].mean()))\n",
    "    print(\"Std RMSE: \" + str(np.std(testScores[key])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3410b0f79b8c75271774ffecaaeec3a3271a520ea70877035cffb4c11d5a35c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
